\documentclass{article}

% general packages:
\usepackage[utf8]{inputenc}		% use Unicode
\usepackage{tipa} 				% for IPA
\usepackage{tikz}				% for Diagrams
	\usetikzlibrary{positioning}       	% for relative positioning 
\usepackage{graphicx}			% for inserting images
	\graphicspath{ {./} }			% the path for the images
\usepackage{hyperref}			% for links

% packages for Dynamic OT:
\usepackage{pifont}			         % for pointing hand
\usepackage{arydshln}			% for dashed lines
\usepackage{rotating}			% for angled text

% more packages:
\usepackage{multirow} 			% for tables
\usepackage{enumitem} 			% for ordered list a-b-c 
\usepackage{fancyhdr}			% for header in each page
\pagestyle{fancy}

\begin{document}

\begin{titlepage}
    \begin{center}
	\vspace*{2cm}
        	\huge
        	\textbf{Learning Head-Complement Order with Minimalist Grammars \newline \\ Using a genetic algorithm}


            
        	\vspace{2cm}
        	\LARGE
        	\textbf{Roy Mayan}\\
       	Submitted to: Prof. Roni Katzir 
            
        	\vfill
          
	Presented as a final paper of\\
	Parsing Seminar: Computation and Cognition
            
        	\vspace{0.8cm}
            
        	\Large
       	Department of Linguistics\\
       	Tel Aviv University\\
        	July 2024
    \end{center}
\end{titlepage}

% FUTURE: I wish to thank the TAU Computational Linguistics Lab people for helping with the arrangement of this paper (especially the coding sections)   , and my dear friend Maya for her input and proofing of the paper. 

% -------------------------------------------------------
% 0 - Abstract
% -------------------------------------------------------
\section*{Abstract}
{\sffamily\small
Head-Complement order exhibits variability both across languages and within them. Several theoretical frameworks have been proposed to elucidate its mental representation. However, all current theories concur that a fundamental aspect of this order must be acquired - either the order itself or some mechanism facilitating appropriate movement. An interesting method for exploring and comparing these theories is using a computational learning perspective, and first steps toward this comparison were made by Avraham (2017). \\
However, current results show the learner sometimes prefers the "wrong" grammar over the expected grammar. In this paper, I implemented a genetic based algorithm for Avraham's learner, providing a different approach for finding the best grammar during the learning process. The purpose is to improve the learning process, and have the learner arrive at the expected grammar, also more efficiently. This transition is the initial step in this direction.Hopefully, this will enable us to advance to the next phase, where comparisons can be made between the different theories from this computational perspective.
%The results show.
}

% -------------------------------------------------------
% 1 - Introduction
% -------------------------------------------------------
\section{Introduction} 
In order to compare different theories regarding Head-Complement order issue from a computational perspective, several modules need to be developed. Such framework was presented by Avraham utilizing Minimalist Grammars (2017),  in which a learner starts from an initial hypothesis (grammar), and based on a given input learns the best grammar that fits it. The items that comprise the grammar that the learner chooses, are configurable in order to test the different theories (for example, whether we assume movement is possible or not). This framework indeed showcases a successfull learning process for the different theories, in most cases and on basic linguistic inputs. Yet,  there several improvements that can be taken in order to make the original framework better and serve as a codebase for a deeper analysis of this phenomenon (including more complex syntatic inputs). \\

The goal of this paper is to revive the current learning framework, while focusing on the algorithm that leads to the best grammar during the learning process, suggesting a genetic algorithm (henceforth GA), over the currently used simulated annealing (henceforth SA). Furthermore, current codebase needs adapation and refactoring, to render it easier for future development regarding this syntactic issue.
In section 2, I introduce the theoretical background (Head-Complement order and previous work),  then I present my proposal for improving the learner and explain its motivation.  Then,  in section 3,  I present the manner of implementation of the genetic algorithm. Finally,  in section 4,  I summarize the conclusions that I draw from my work.

% -------------------------------------------------------
% 2 - Background (Avraham 1, 2, 3)
% -------------------------------------------------------
\clearpage
\section{Background} 
In linguistics, the study of languages, phonology is the study of their pronunciation,  the sound system of a language and its properties.

\subsection{Head-Complement Order} % (Avraham 1 - Head-Complement order)

First, the 
% FUTURE: History in Linguistic literature - expand more on the history and everything Roni mentioned in parsing seminar, but it's not relevant now, only for the second part of the paper in learning seminar, when we can actually begin comparing the theories

\subsection{Terminology and basic definitions} % (Avraham 1 - Head-Complement order)
First, the 
\begin{enumerate}
  \item \textbf{A}: TBD
  \item \textbf{Universal Grammar (UG)}: The premise of Generative Linguistics is the fact that native speakers have knowledge about
their language that allows them to produce and comprehend words and sentences that they have never heard before. The description of that knowledge is called a Grammar, and it is what humans are assumed to acquire when acquiring language. This led to the thought that humans are born with some characterization of the set of grammars that they are able to acquire,  which Chomsky proposed to call UG.
\end{enumerate}

\subsection{Theoretical Models for Learning} % (Avraham 2 - Theoretical Models for Learning)
To approach this from a computational learning perspective, several theoretical models were used:
\begin{enumerate}
  \item \textbf{Learning algorithm}: TBD
  \item \textbf{Language class (MCS)}: TBD
  \item \textbf{Formalism (MG)}: TBD
  \item \textbf{Parser}: TBD
  \item \textbf{Metric (MDL)}: TBD
\end{enumerate}

% Current implementation (Avraham 3 - Implementing Head-Direction Learners)
\subsection{Current Implementation} 
These were all implemented by Avraham in the following manner:
\begin{enumerate}
  \item \textbf{Formalism (MG)}: TBD
  \item \textbf{Parser}: Based on (Stabler 1997) and 
  \item \textbf{Metric (MDL)}: TBD
\end{enumerate}

% -------------------------------------------------------
% 3 - Learning algorithm (Avraham 3 - SA)
% -------------------------------------------------------
\clearpage
\section{Learning Algorithm} 
Now we move on to focus on the learning component, more specifically the learning algorithm used.

% following subsections (my proposal)
\subsection{SA background} % (nur)
TBD
\subsection{GA background} % (nur)
% use the word "evolutionary"
TBD
\subsection{GA motivation} % (nur)
- widely used in recent years
- there are multiple evidence for the growing use of GA as part of various tasks in computational linguistics
- phonology (in the lab)
- among these we find grammar induction
% (i) Genetic algorithms (what is it? what is the general structure?)
% (ii) Why choose GA/Use of evolutionary algrithms in statistical NLP bla bla (advantages, why did I choose to go in this direction? based on various articles I have read - צריך לעשות פה סקירות ספרות קטנה ולשכנע למה אני מנסה לממש כזה דבר))

% useful code
\begin{itemize}
  \item A
\end{itemize}

\begin{description}
   \item[Iterativity] This example
\end{description}

\subsection{Proposal} % my proposal and its motivation
As previously stated, the data regarding OSD which was cited many times later, comes from Dell's work. As of now, we do not know exactly to what extent his generalisations are correct, and whether French speakers do prefer these patterns in different combinations of words.

My main research question is whether speakers of French are willing to accept the patterns Dell suggested in his analysis of OSD back in 1973.  Do they prefer the opaque (caused by self-counterbleeding) or transparent (caused by self-bleeding) forms?

Another question is related to the direction in which the rule applies. As mentioned,  Dell's [kstr] example does favor left-to-right, but is this what happens in most cases? Are there any counter-examples?\\

The importance of knowing whether Dell's suggestions are characteristic of speakers today is directly related to its historical significance in the discussion over different theories of phonology,  as explained above (the connection between iterativity and opacity, locality, optionality) - that is the motivation for this study. I will try to address this question using an experiment, as described in detail in the following section.

% -------------------------------------------------------
% 4 - Main: Implementing GA and running the experiments
% -------------------------------------------------------
\clearpage
\subsection{Refactoring}
As I took the original code base written by Avraham, modifications and adjustments have been made in order to allow further and more modern development. The main changes committed are:
\begin{itemize}
  % HCSL-1
  \item Upgrading the environment from python 2.7 to python 3.11.
  \item Packging the different modules to make the project more coherent, and especially the learning modules more independent and interchangeable.
  % HCSL-3  
  \item Adding proper configuration for the project to allow better control over different experiments and learning options.
\end{itemize}

\section{GA Implementation} 
In order to test native French speakers' knowledge of their language to verify Dell's analysis, they were exposed to recordings of different pronunciation options and asked to rate them.  For online data collection,  the software PsyToolkit was used (Stoet, 2010, 2017). After constructing the basic phrases myself, I consulted a French speaker who also recorded in her voice all the different versions as described below. 
% How do I implement a GA here? How do I define the different objects needed?
% How do I incorporate it in the given code base?
% Based on the GA class from Matan, now we are left to define the function - fitness, mutation etc.


% building 
\subsection{Experiment}
The pairs of recordings were formed as 15 questions in the online survey, where for each pair of recordings "A" and "B" there was a seperate scale: from 1 (pas d'accord) to 7 (d'accord) - that is, 1 is the lowest grade of acceptance of the pronunciation, and 7 is the highest grade. After having built the full survey,  I shared it online using the help of French speakers, who forwarded it to more people.  
Regarding the responses I received:
\begin{enumerate}
  \item A total of 22 subjects.
  \item All the subjects are native French speakers - mostly from France, but also from Belgium, Israel,  and Asia.
  \item The subjects’ ages ranged from 21 to 65 with an average of 29.22.
\end{enumerate}

\subsection{Expectations}
From conversations I had with about 3-4 French speakers regarding the OSD issue whilst preparing the experiment,  I got the impression that on the whole they agree with Dell's proposition; The default option is clearly valid. Deleting either schwa is optional and completely valid (though considered as informal speech), and the tendency to delete the left or right schwa depends on the specific French dialect, region etc. In this humble experiment I ignored these factors (which were researched for example by Bayles \& Kaplan (2016)), and focused on the general acceptance of the recordings with random French speakers. 
Regarding the fully deleted versions - most speakers agreed they sound bad, though one pointed out he thinks recording 4 is not completely wrong (he can imagine French speakers producing it, and stated he would understand the meaning with no effort). 

Therefore, I expect a general correlation with Dell's analysis: phrases 1-3,6-8,11-13,15-17 being correct. In addition, I expect some variation across phrases 4 and 9 which can be easily pronounced, and maybe not completely wrong as Dell treated them. 

\clearpage
% -------------------------------------------------------
% 5 - Conclusions
% -------------------------------------------------------
\section{Conclusions}

\subsection{Results and analysis}
There are many ways to analyze the results of the experiment. Given the clear division into pairs using the previously defined categories (N, R, L , D) it will be interesting to see the results of the same categories across different phrases.  Also,  I'm interested in looking for the amount of consistency regrading the same pair/phrase.  Here are the main points that came up from the data:
\begin{enumerate}
  \item The N version (no deletion at all) given in phrases 1,5,6,10,11,15 got the average score of \textbf{6.6} as expected.
\end{enumerate}

\subsection{Possible problems and recommendations}
\begin{enumerate}
  \item I chose sentences with only 2 schwas, but it will be interesting to test more complicated cases with 3 or even 4 schwas (such examples were given by Dell himself).
\end{enumerate}

\subsection{Discussion}
The OSD pattern in French is an interesting phenomenon due to its popularity and significance in literature. The generalizations given by Dell to explain the OSD logic seem to be characteristic of French speakers today. The results of this small experiment I conducted support Dell's propositions, hence they support the analysis of the VCE rule application as left-to-right. Speakers seem to have strong intuitions about this phenomenon.
My conclusions are certainly tentative because of the previously mentioned confounds in the experiment,  that should be addressed in a more extensive follow-up experiment (given my recommendations).



% step one - genetic instead of simulated annealing
% step two - a learner that works in several steps
% step three - adding more syntactic structures in addition to coordination (topicalization & questions)
% step four - frequency analysis
I suggest the following steps that can be taken in order to continue improving this framework:
- Parser / MG maybe change the formalism
- Avraham suggestions for improving the learner from 2017 % (maybe ask Roni, are they still really plausible? relevant?)

% FUTURE: what Roni brought up several times - parser/learner for morphology (distributed morphology) combining my work (the final version after learning seminar), not relevant to mention now

% -------------------------------------------------------
% Bibliography
% -------------------------------------------------------
\clearpage
\fancyhead{} % clear all header fields
\fancyhead[RO]{\textit{REFERENCES}}

\begin{thebibliography}{} % - References

% the main work I base on
\bibitem{texbook} 
Avraham, T. (2017). 
Learning Head-Complement Order with Minimalist Grammars.
MA thesis, Tel Aviv University.

% section 1
\bibitem{texbook}
Chomsky, N. (1981). 
Lectures on Government and Binding.
Dordrecht: Foris.

\bibitem{texbook}
Cinque, G. (2017). 
A microparametric approach to the head-initial/head-final parameter. 
Linguistic Analysis, 41(3-4), 309-366.

\bibitem{texbook}
Kayne, R. S. (1994). 
The antisymmetry of syntax (Vol. 25). 
MIT press.

% section 2
\bibitem{texbook}
Harkema, H. (2001). 
Parsing minimalist languages. 
University of California, Los Angeles.

\bibitem{texbook}
Katzir, R. (2014). 
A cognitively plausible model for grammar induction. 
Journal of Language Modelling, 2(2), 213-248.

\bibitem{texbook}
Stabler, E. (1996). 
Derivational minimalism. In International conference on logical aspects of computational linguistics (pp. 68-95). 
Berlin, Heidelberg: Springer Berlin Heidelberg.

% section 3 - web
\bibitem{texbook}
Araujo, L. (2007). 
How evolutionary algorithms are applied to statistical natural language processing. 
Artificial Intelligence Review, 28, 275-303.

\bibitem{texbook}
Partlan, R. (2023). 
Using Genetic Algorithms for Unsupervised Context-Free Grammar Induction.
Doctoral dissertation, Brandeis University.

% section 3 - tau comp lab, mostly phonology
\bibitem{texbook}
Lan, N. (2018). 
Learning morpho-phonology using the Minimum Description Length Principle and a Genetic Algorithm.
Master’s thesis, Tel Aviv University.

\bibitem{texbook}
Rasin, E., Berger, I., Lan, N.,  Katzir, R. (2018). 
Learning phonological optionality and opacity from distributional evidence. 
In Proceedings of NELS (Vol. 48, pp. 269-282).

\bibitem{texbook}
Rasin, E., Berger, I., Lan, N., Shefi, I.,Katzir, R. (2021). 
Approaching explanatory adequacy in phonology using Minimum Description Length. 
Journal of Language Modelling, 9(1), 17-66.

% section 4 - things from section 3 I guess

\end{thebibliography}

\end{document}